# HW1. Experiment management
## Задача
Нужно провести эксперимент с обучением модели, классифицируя комментарии комментарии по приципу IsHate/notIsHate. В качестве инструментов необходимо испольльзовать:
1) Любая модель машинного обучения
2) Pandas или polars для работы с данными
3) Hyperopt или Optuna для поиска оптимальных гиперпараметров
4) DVC для версионирования данных и результатов эксперимента

## Code
Код хранится в py - файлах: preparing.py (подготовка данных), training.py (обучение модели и подбор гиперпараметров), evaluating.py (сравнение моделей и сохранение метрик). 
В файле params.yaml находятся параметры для запуска скриптов. 
Также для удобства процесс предобработки данных, обучения моделей и оценки представлен в ноутбуке HW1.ipynb.
Пайплайн обучения запускается скриптом launch_sc.sh, которые находится в папке с кодом.

## Dataset
Датасет содержит 998 комментариев с разной степенью токсичности. 
Изначально в колонке isHate есть значения между 0 и 1, которые были преобразованы в 0 или 1 по принципу больше или меньше это значение чем 0.5.

![image](https://github.com/Mostovik71/dlframe/assets/56130198/0a50af5b-3643-4ae0-a8bf-e8d8c085418d)

После преобразования датасет разбивается на train и test часть с размерами 0.7 и 0.3 от исходного соответственно.

## Model Selection
В качестве feature - extractor'а был выбран TfidfVectorizer из библиотеки sklearn со следующими основными параметрами: 
ngram_range = (1, 2), 
max_features = 2000.

В качестве основной модели был выбран LGBMClassifier из библиотеки lightgbm с параметром random_state = 42.

## Hyperparameter Optimization
В качестве библиотеки для оптимизации гиперпараметров была выбрана HyperOpt. Параметры для метода fmin и оптимизируемой функции: max_evals = 5, algo = tpe.suggest, cv = 3, metric = 1 - roc_auc.

Поле для поиска гиперпараметров: 

"max_depth": hp.choice('max_depth', np.arange(4, 12, 2, dtype=int)) - глубина дерева

"learning_rate": hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)) - скорость обучения

"feature_fraction": hp.uniform('feature_fraction', 0.5, 1) - количество фичей для построения каждого дерева 

"num_leaves": hp.choice('num_leaves', np.arange(16, 256, 2, dtype=int)) - максимальное количество листьев в дереве

## Experiment Tracking
Датасет и результаты экспериментов логируются при помощи dvc и сохраняются на google-drive. Также все модели, найденные гиперпараметры, тренировочные и тестовые данные, метрики сохраняются в json - файлы (в папку logs) и логируются при помощи библиотеки logger.



